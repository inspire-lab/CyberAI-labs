{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inspire-lab/CyberAI-labs/blob/main/Intrusion-detection/intrusion_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnSGnCWWFsMB"
      },
      "source": [
        "# Lab 2\n",
        "## Intrusion detection and prevention\n",
        "In this lab we will use different methods to detect intrusions which can be used to prevent intrusions. We will experiment with numeric data, traces and time information rather than text. The goal of this lab is to learn to handle datasets containing large features. We will use technics to automatically combine, rank and select features. We will evaluate the performance of the model with selected features and compare it with a model that includes all features.\n",
        "\n",
        "The dataset used in this lab is adapted from CICIDS dataset available <a href=\"https://www.unb.ca/cic/datasets/ids-2017.html\">here.</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP7oOc_IFsMI"
      },
      "source": [
        "Let us first set up a simple filter to ignore the future warnings and deprecation warnings we may encounter during the execution of this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5SfOTH8FsMK"
      },
      "outputs": [],
      "source": [
        "# import warnings filter\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "\"\"\"from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "simplefilter(action='ignore', category=DeprecationWarning)\n",
        "#simplefilter(action='ignore', category=ConvergenceWarning)\"\"\"\n",
        "if not sys.warnoptions:\n",
        "  warnings.simplefilter(\"ignore\")\n",
        "  os.environ[\"PYTHONWARNINGS\"] = \"ignore\"  # Also affect subprocesses|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wyY-wzJFsMN"
      },
      "source": [
        "The first step to train any model in a machine learning task is to load the data. The data needed for this lab is present in the `Friday-WorkingHours-Morning.pcap_ISCX.csv` file, where each row indicates an instance of BENIGN request or BOT request in a network. Read more about the dataset <a href=\"https://www.unb.ca/cic/datasets/ids-2017.html\">here.</a>\n",
        "\n",
        "After loading the dataset, we assign X and Y variables to features and labels resepctively. Features are characteristics observed for a particular label (in this case BENIGN or BOT activity) when the dataset was created.\n",
        "\n",
        "The ' Labels' column in the dataset is assigned to Y variable and the features present in the rest of the columns are transformed using an imputer function. This imputer function is useful in handling any missing values in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "EjJvwv7NFsMO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"data/Friday-WorkingHours-Morning.pcap_ISCX.csv\")\n",
        "\n",
        "Y = df[' Label']\n",
        "del df[' Label']\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X = df.dropna(axis=1, how='all')\n",
        "columns = df.columns.values.tolist()\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean', axis=0)\n",
        "imputer = imputer.fit(df)\n",
        "X = imputer.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAg91nzMFsMP"
      },
      "source": [
        "## Question 1:\n",
        "Print the number of features present after transformation using the imput function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "kIZNsKqIFsMQ"
      },
      "outputs": [],
      "source": [
        "# solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRzhevEuFsMR"
      },
      "source": [
        "## Question 2:\n",
        "Create a 80/20 train test split using X as features and Y as labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEvJLjEiFsMS"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVHynulGFsMT"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prR6lbmjFsMU"
      },
      "source": [
        "Let us now print the number of labels in each category in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "qQPSYRNeFsMU"
      },
      "outputs": [],
      "source": [
        "BENIGN_counts, bot_counts = y_train.value_counts()\n",
        "print(BENIGN_counts)\n",
        "print(bot_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeMZfkRtFsMV"
      },
      "source": [
        "We can clearly see that the number of BENIGN labels out number the number of Bot labels. This indicates that the dataset is unbalanced. Let us now balance our dataset using undersampling. This technique underrepresents the majority class (BENIGN in our case) to balance the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laxsK0SNFsMW"
      },
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "\n",
        "rus = RandomUnderSampler(random_state=89)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
        "print(sorted(Counter(y_resampled).items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1G4isbQFsMW"
      },
      "source": [
        "We can now see that our dataset is balanced. Let us now consider our undersampled train dataset to be our actual training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouGoddBUFsMW"
      },
      "outputs": [],
      "source": [
        "X_train = X_resampled\n",
        "y_train = y_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCRJf1tOFsMX"
      },
      "source": [
        "Let us now build a simple logistic regresstion model.\n",
        "\n",
        "## Question 3:\n",
        "Using the logmodel predict the labels for X_test and print the classification report for y_test and predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "LOVEdpF4FsMX"
      },
      "outputs": [],
      "source": [
        "# solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2ejYLyNFsMY"
      },
      "source": [
        "## KBest:\n",
        "The kbest algorithm is used to pick the k best features from a given dataset using a function that maps the features and the labels. The features are ranked using the function and all features ranking greater than k are removed. Read more on sklearn's kbest implementation <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\">here.</a>\n",
        "\n",
        "The output of this code block prints the chosen k features from the top 5 rows in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "2r_pxT4gFsMY"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "test = SelectKBest(score_func=f_classif, k=25)\n",
        "kbest_fit = test.fit(X, Y)\n",
        "# summarize scores\n",
        "np.set_printoptions(precision=3)\n",
        "#print(kbest_fit.scores_)\n",
        "kbest_features = kbest_fit.transform(X)\n",
        "# summarize selected features\n",
        "print(kbest_features[0:5, :])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci_eHGBAFsMY"
      },
      "source": [
        "Let us now build a logistic regression model only with the chosen features from the kbest algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "mF_fgqFZFsMZ"
      },
      "outputs": [],
      "source": [
        "X_train_kbest, X_test_kbest, y_train_kbest, y_test_kbest = train_test_split(\n",
        "    kbest_features, Y, test_size=0.30, random_state=101)\n",
        "logmodel.fit(X_train_kbest, y_train_kbest)\n",
        "predictions = logmodel.predict(X_test_kbest)\n",
        "print(classification_report(y_test_kbest, predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8FGS5gQFsMZ"
      },
      "source": [
        "## Question 4:\n",
        "Comment on the performance of the model build using only the k best features versus the model build using all the features. Why do you think there is a difference in the performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haYoJICoFsMZ"
      },
      "source": [
        "## Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLLwVAunFsMZ"
      },
      "source": [
        "## RFE:\n",
        "RFE refers to recursive feature elimination. RFE recursively removes the weakest features until the mentioned number of features is reached. Learn more about sklearn's RFE implementation <a href =\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\">here.</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "M1UWnoiZFsMa"
      },
      "outputs": [],
      "source": [
        "rfe = RFE(logmodel, n_features_to_select=10)\n",
        "rfe_fit = rfe.fit(X, Y)\n",
        "#print number of features chosen\n",
        "print(rfe_fit.n_features_)\n",
        "#print if each feature from the dataset is chosen or not\n",
        "print(rfe_fit.support_)\n",
        "#print the ranks of each feature\n",
        "print(rfe_fit.ranking_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82i9wn96FsMa"
      },
      "source": [
        "## Question 5:\n",
        "Build a logistic regression classifier with the features obtained from RFE. Print the classification report to evaluate the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnYGymYeFsMa"
      },
      "outputs": [],
      "source": [
        "# solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhlzCkmGFsMb"
      },
      "source": [
        "## Question 6:\n",
        "Evaluate the performance if the number of chosen features is decreased to 3. What happens if the number of chosen features is 25? What happens if the  number of chosen features is 50? Does increase in the number of features affect the performance of the model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2qPctY4FsMb"
      },
      "source": [
        "## Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFq7-iKaFsMb"
      },
      "source": [
        "## PCA:\n",
        "Principal Component Analysis is a statistical procedure that takes in observations that are possibly correlated to generate non-correlated values called the principal components. Read more on sklearns PCA implementation <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">here.</a>\n",
        "\n",
        "This code block prints the chosen number of principal components generated using the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "K8BsIyBBFsMb"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=5)\n",
        "pca_fit = pca.fit(X)\n",
        "print(pca_fit.components_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNg0CsmkFsMc"
      },
      "source": [
        "## Question 7:\n",
        "Try 10, 25 and 50 principal components as input and evaluate their performance by looking at their classification report. Print the classification report for which the maximum performance is achieved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9e5Wanh0FsMc"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4XUfPI6FsMd"
      },
      "source": [
        "## Decision trees:\n",
        "Decision trees are non-parametric algorithms that exploit the features in a dataset to generate a tree based model. Let us build a decision tree for our dataset and evaluate its performance. Read more on sklearn's decision tree implementation <a href=\"https://scikit-learn.org/stable/modules/tree.html\">here.</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em1DC47pFsMd"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier  # Import Decision Tree Classifier\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "clf = clf.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "predictions = clf.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO_uCw4WFsMd"
      },
      "source": [
        "Let us now build a decision tree where we restrict the maximum depth of a tree. This allows the decision tree classifier to leave out features that are not very important in the classification task, and to control the size of the tree.\n",
        "\n",
        "## Question 8:\n",
        "From the sklearn's implementation of decision trees, pass the parameter that specifies the maximum depth of the tree. Assign maximum depth of the tree to 3 and evaluate its performance by printing a classification report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aElhugOXFsMe"
      },
      "outputs": [],
      "source": [
        "# solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8K50Ud7FsMe"
      },
      "source": [
        "## Question 9:\n",
        "From the sklearn's implementation of decision trees, pass the parameter that specifies the minimum number of leaves in the tree. Assign the minimum number of leaves in the tree to 3 and evaluate its performance by printing a classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y-tIVhcFsMm"
      },
      "outputs": [],
      "source": [
        "# solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9EFJTQhFsMm"
      },
      "source": [
        "In this lab we learned to automatically extract features from a large number of features. From the results, we can see that the performance of models built on the selected features is comparable with the one built on the entire dataset."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}