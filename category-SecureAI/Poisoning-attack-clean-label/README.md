# Clean Label Poisoning Attacks

This lab explores clean label poisoning attacks, where adversarial examples are carefully crafted to appear benign while subtly corrupting the training process. Students will implement a clean label attack using a substitute model and evaluate how adversarial examples transfer to the target classifier. The lab highlights how attackers can bypass conventional data validation while still embedding malicious behavior into machine learning models.
