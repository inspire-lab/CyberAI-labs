{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inspire-lab/CyberAI-labs/blob/main/category-SecureAI/Hate-speech-offensive-language-classification/hatespeech_Vs_offensive_language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pECBFvCjvRpH"
      },
      "source": [
        "# Lab 4\n",
        "## Classifying between hate speech and offensive language.\n",
        "\n",
        "In this lab we will learn to classify hate speech versus offensive language. Specifically, we will use a social media corpus consisting of tweets which are labeled as hate_speech, offensive_language or neither.\n",
        "\n",
        "We will try an approach similar to what we followed in Lab 1 to see how the results are. Then, we will understand what word embeddings are, and use the popular glove pre-trained word embeddings in our model. Later, we will train our own word embeddings to use in our model and compare the results between the two.\n",
        "\n",
        "The task itself is not as straightforward as it might appear at first glance. Just think about it. There is a big difference between between `You are f****ing great` and `You are f****ing @*%&!*@#%` (replace `@*%&!*@#%` with your favorite worst insult). We can all agree on the first sentence not being hate speech. But some people might consider offensive language. The problem is very subjective.\n",
        "\n",
        "This is reflected in the data labeling. The creators of the data set gave the tweets to multiple annotators (Annotators are people who assign labels to data, roughly speaking) and the label would be what the most annotators asigned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZTfZyJxvRpL"
      },
      "outputs": [],
      "source": [
        "# this lab needs tensorflow 2.8.0\n",
        "!pip install tensorflow==2.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAwL-iCfvRpN"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "Let us first understand our task. Given the tweets below,categorize eachof these tweets into one of the category\n",
        "#### -hate_speech\n",
        "#### -offensive_language\n",
        "#### -neither\n",
        "\n",
        "1. \"@Blackman38Tide: @WhaleLookyHere @HowdyDowdy11 queer\" gaywad\n",
        "\n",
        "2. \"I'll slit your throat-neck\"\n",
        "\n",
        "3. \"Roberto you are one nasty hoe\"\n",
        "\n",
        "4. &#8220;@CommonBlackGirI: here&#8217;s to all the kids who have never found their name on anything in a souvenir store&#8221; thanks mom for the ghetto name.\n",
        "\n",
        "5. &#8220;@Damn_ImTall: Anybody that gets traded straight up for Brandon knight is trash @BrandonJennings&#8221;@BushALMIGHTY\n",
        "\n",
        "6. &#8220;@DickFurari: HELL YA THEY SHOULD MAKE MORE MALT LIQUOR ADS LIKE THIS. http://t.co/H0JGORtnF8&#8221;- Dis be heaven fo a colored man"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raBJ-ARjvRpN"
      },
      "source": [
        "Your Answers:\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n",
        "4.\n",
        "\n",
        "5.\n",
        "\n",
        "6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEoQp0NLvRpN"
      },
      "source": [
        "You would have now understood how ambiguous it is to decide the label of each tweet is. To see what the labels in the dataset are, scroll to the bottom of this page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSyyMRdZvRpO"
      },
      "source": [
        "The first step is to load the data into a data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-1GxhtfbvRpO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "col_names = [\n",
        "    'sno', 'count', 'hate_speech', 'offensive_language', 'neither', 'class',\n",
        "    'tweet'\n",
        "]\n",
        "df = pd.read_csv('data/hate_offensive.csv', skiprows=[0], names=col_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs3NKZYKvRpO"
      },
      "source": [
        "Now we define a function tweet_cleaner to preprocess the tweets.\n",
        "\n",
        "## Question 2:\n",
        "The function tweet_cleaner misses the statement to convert the tweets to lower case. Insert the required code in the specified location. Look at what else the function does."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# solution\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tok = WordPunctTokenizer()\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "combined_pat = r'|'.join((pat1, pat2))\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "  soup = BeautifulSoup(text, 'lxml')\n",
        "  souped = soup.get_text()\n",
        "  stripped = re.sub(combined_pat, '', souped)\n",
        "  try:\n",
        "    clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "  except:\n",
        "    clean = stripped\n",
        "  letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "  #######################\n",
        "  lower_case = # complete the code\n",
        "  #######################\n",
        "  lower_case = letters_only.lower()\n",
        "  words = tok.tokenize(lower_case)\n",
        "  return (\" \".join(words)).strip()"
      ],
      "metadata": {
        "id": "4x56I8bcx-07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_oNQhVmvRpO"
      },
      "source": [
        "Call the function tweet_cleaner to clean the tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ko5LDlDivRpO"
      },
      "outputs": [],
      "source": [
        "df['tweet'] = df['tweet'].apply(tweet_cleaner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0sD-CeevRpP"
      },
      "source": [
        "The dataset we use has around 25000 tweets. It will take a long time to train during the lab session, given our system configuration. So we will use the top 2000 tweets for our training. Print the top five tweets to see how the preprocessed tweets look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "-16MccE2vRpP"
      },
      "outputs": [],
      "source": [
        "print(df.tweet.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44iDV3xRvRpP"
      },
      "source": [
        "## Question 3:\n",
        "Create a 80:20 train test split from the dataframe df."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# solution\n",
        "from sklearn.model_selection import train_test_split\n",
        "#######################\n",
        "train, test = # complete the code\n",
        "#######################\n",
        "train, test = train_test_split(df, test_size=0.2, stratify=df['class'])\n",
        "targets = train['class']"
      ],
      "metadata": {
        "id": "W0vedgN6yLp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtoTOcqQvRpP"
      },
      "source": [
        "Let us prepare our tweets for the bag of words model and then set up a majority baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bdYEPCv0vRpP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# CountVectorizer-Convert a collection of text documents to a matrix of token counts\n",
        "count_vectorizer = CountVectorizer(analyzer='word', max_features=500)\n",
        "train_bog = count_vectorizer.fit_transform(train['tweet'].values)\n",
        "test_bog = count_vectorizer.transform(test['tweet'])\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "base = DummyClassifier(strategy='most_frequent')\n",
        "base.fit(train_bog, targets)\n",
        "base_predictions = base.predict(test_bog)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVFyiADBvRpP"
      },
      "source": [
        "## Question 4:\n",
        "Print the classification report for the majority baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Kqqz4v8QvRpP"
      },
      "outputs": [],
      "source": [
        "# solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5vuG2h9vRpP"
      },
      "source": [
        "## Question 5:\n",
        "Implement a naive bayes bag of words model using the given train data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#######################\n",
        "# complete the code\n",
        "\n",
        "# holds the predictions of the naive bayes bag of words model\n",
        "bog_bayes_predictions =\n",
        "#######################\n",
        "\n",
        "print(\n",
        "    classification_report(test['class'],\n",
        "                          bog_bayes_predictions,\n",
        "                          labels=[0, 1, 2]))\n"
      ],
      "metadata": {
        "id": "SOfGBAubycDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhQZYK96vRpQ"
      },
      "source": [
        "## Question 6:\n",
        "Implement a naive bayes TF-IDF model using the train data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#######################\n",
        "# complete the code\n",
        "\n",
        "# holds the predictions of the tf-idf model\n",
        "tfidf_bayes_predictions =\n",
        "#######################\n",
        "print(\n",
        "    classification_report(test['class'],\n",
        "                          tfidf_bayes_predictions,\n",
        "                          labels=[0, 1, 2]))\n"
      ],
      "metadata": {
        "id": "caiGESkzykE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGTjhGkrvRpQ"
      },
      "source": [
        "## Question 7:\n",
        "Implement a naive bayes bigram model using the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kI5fhVdfvRpQ"
      },
      "outputs": [],
      "source": [
        "# solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C-gVnsmvRpQ"
      },
      "source": [
        "Now that we have tried the traditional learning methods, let us try some neural learning architectures using word embeddings.\n",
        "We will use keras to implement neural network architectures. Read more on keras <a href = \"https://keras.io/\"> here </a>."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.3"
      ],
      "metadata": {
        "id": "Z-UOZ1K_ywYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kXd5AR-evRpQ"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMLRZgIjvRpQ"
      },
      "source": [
        "Before we can use the embeddings we need to download the pretrainted word vectors. https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkrCbwxqvRpQ"
      },
      "outputs": [],
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNvMT2GrvRpQ"
      },
      "source": [
        "Word embeddings are a representation of words as numbers, as neural networks can understand only numbers. Learn more about word embeddings <a href=\"https://machinelearningmastery.com/what-are-word-embeddings/\">here</a>.\n",
        "\n",
        "The code below loads the glove embeddings into a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WPlxC9ftvRpQ"
      },
      "outputs": [],
      "source": [
        "embeddings_index = dict()\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVKwTPIqvRpQ"
      },
      "source": [
        "In the code block below, we tokenize the tweets in our dataset and create indices for each word in the vocabulary. We then make the length of the tweets equal.To do so, we will the length of the longest tweet nd append all other tweets with padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTd3cd7TvRpQ"
      },
      "outputs": [],
      "source": [
        "# download additional ressrouces\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Xk75vngVvRpQ"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tweet = df.tweet\n",
        "word_count = lambda tweet: len(word_tokenize(tweet))\n",
        "\n",
        "max_len_tweet = len(max(tweet, key=word_count))\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(tweet)\n",
        "vocabulary_size = len(t.word_index) + 1\n",
        "encoded_tweet = t.texts_to_sequences(tweet)\n",
        "padded_tweets = pad_sequences(encoded_tweet,\n",
        "                              maxlen=max_len_tweet,\n",
        "                              padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQoPsrk2vRpQ"
      },
      "source": [
        "We will create an embedding matrix which will load the glove embeddings for the words in our vocabuary from the dictionary embeddings_index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xOQh58cCvRpQ"
      },
      "outputs": [],
      "source": [
        "embedding_size = 100\n",
        "embedding_matrix = np.zeros((vocabulary_size, embedding_size))\n",
        "for word, index in t.word_index.items():\n",
        "  if index > vocabulary_size - 1:\n",
        "    break\n",
        "  else:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[index] = embedding_vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKsrRkzVvRpQ"
      },
      "source": [
        "Clearly, we have loaded the embeddings for the entire dataset.\n",
        "\n",
        "## Question 8:\n",
        "Create a 80:20 train test split over the dataset that are represented as the indices in the embedding_index dictionary. Ensure that the train test instances in the previous split is the same as in the current split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6rdeAcirvRpR"
      },
      "outputs": [],
      "source": [
        "# solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz1mdsqmvRpR"
      },
      "source": [
        "Let us now create a simple model using the glove embeddings with one LSTM layer. Do not worry about the parameters of the architecture for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wZZB0XQsvRpU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "\n",
        "model_glove = Sequential()\n",
        "model_glove.add(\n",
        "    Embedding(vocabulary_size,\n",
        "              embedding_size,\n",
        "              weights=[embedding_matrix],\n",
        "              input_length=max_len_tweet,\n",
        "              trainable=False))\n",
        "model_glove.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n",
        "model_glove.add(Dropout(0.3))\n",
        "model_glove.add(Dense(3, activation='softmax'))\n",
        "model_glove.compile(loss='categorical_crossentropy',\n",
        "                    optimizer='adam',\n",
        "                    metrics=['accuracy'])\n",
        "model_glove.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hucgknhvRpU"
      },
      "source": [
        "Let us train the model using a 20% validation split from the training set. The objective is to increase the accuracy and decrease the loss. Play with the different hyper parameters (epochs, batch_size) to see if there is a change in the accuracy and loss values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0ZKq4adkvRpU"
      },
      "outputs": [],
      "source": [
        "model_glove.fit(train_x,\n",
        "                train_y,\n",
        "                validation_split=0.2,\n",
        "                epochs=2,\n",
        "                batch_size=32,\n",
        "                verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5QKSkCV3vRpU"
      },
      "outputs": [],
      "source": [
        "predictions = model_glove.predict(test_x)\n",
        "predictions = to_categorical(np.argmax(predictions, axis=1), num_classes=3)\n",
        "print(classification_report(test_y, predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrTz67SbvRpU"
      },
      "source": [
        "To examine if our model performs better with a different configuration, we can increase the number of epochs. However, this could also increase the run time. Hence we will load the model which used 20 epochs instead of 2 epochs over the same architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "atYO5PXXvRpU"
      },
      "outputs": [],
      "source": [
        "#loading the saved gloved model with 20 epochs\n",
        "loaded_glove_model = load_model('glove_models/glove_model.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o_IXWQi1vRpU"
      },
      "outputs": [],
      "source": [
        "predictions = loaded_glove_model.predict(test_x)\n",
        "predictions = to_categorical(np.argmax(predictions, axis=1), num_classes=3)\n",
        "print(classification_report(test_y, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pox1J08vRpU"
      },
      "source": [
        "The classification report shows that even after running for 20 epochs, there is no improvement in performance, and all tweets are labeled as offensive_langugae which is our majority label.\n",
        "This indicates that either the architecture of the neural network is too simple, or the way we represent the input (word embeddings) needs modification, or we need further hyper-parameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCoV3dsxvRpU"
      },
      "source": [
        "Let us now train our own word embeddings using the Word2Vec function available in the package gensim. Read more abour Word2Vec <a href=\"https://skymind.ai/wiki/word2vec\">here</a>. Read more on Word2Vec on gensim <a href=\"https://rare-technologies.com/word2vec-tutorial/\">here</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "u-Tzf5gAvRpV"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "df['tokenized_tweet'] = df[\"tweet\"].apply(word_tokenize)\n",
        "\n",
        "gensim_model = Word2Vec(df['tokenized_tweet'],\n",
        "                        vector_size=100,\n",
        "                        min_count=1,\n",
        "                        window=5,\n",
        "                        epochs=100)\n",
        "gensim_trained_weights = gensim_model.wv.vectors\n",
        "gensim_vocabulary_size, gensim_emdedding_size = gensim_trained_weights.shape\n",
        "\n",
        "\n",
        "def word2idx(word):\n",
        "  return gensim_model.wv.vocab[word].index\n",
        "\n",
        "\n",
        "def idx2word(idx):\n",
        "  return gensim_model.wv.index2word[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AngPNNBlvRpV"
      },
      "source": [
        "Let us now create a train test split over the tokenized tweet and then load the embeddings that we trained using Word2Ved in gensim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8tXfQN6BvRpV"
      },
      "outputs": [],
      "source": [
        "# Ensure word2idx uses key_to_index for compatibility with Gensim 4.x\n",
        "def word2idx(word):\n",
        "    # Use .get(word, 0) to return 0 for out-of-vocabulary words\n",
        "    return gensim_model.wv.key_to_index.get(word, 0)\n",
        "\n",
        "# Initialize arrays for train and test data\n",
        "word2vec_train_x = np.zeros([len(train_x), max_len_tweet], dtype=np.int32)\n",
        "word2vec_train_y = np.zeros([len(train_y)], dtype=np.int32)\n",
        "word2vec_test_x = np.zeros([len(test_x), max_len_tweet], dtype=np.int32)\n",
        "word2vec_test_y = np.zeros([len(test_y)], dtype=np.int32)\n",
        "\n",
        "# Encode train set\n",
        "for i, tweet in enumerate(train_x):\n",
        "    for t, word in enumerate(tweet):\n",
        "        if t < max_len_tweet:\n",
        "            word2vec_train_x[i, t] = word2idx(word)  # This should now work without referencing .vocab\n",
        "\n",
        "# Encode test set\n",
        "for i, tweet in enumerate(test_x):\n",
        "    for t, word in enumerate(tweet):\n",
        "        if t < max_len_tweet:\n",
        "            word2vec_test_x[i, t] = word2idx(word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0mp6xjtvRpV"
      },
      "source": [
        "Let us now plot our word embeddings over a graph to understand how the words are related."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GiCktgQDvRpV"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "\n",
        "# Access the vocabulary and vectors\n",
        "vocab = list(gensim_model.wv.index_to_key)  # Use .index_to_key to get the list of words\n",
        "X = gensim_model.wv[vocab]  # Get the vectors for each word in the vocabulary\n",
        "\n",
        "# Perform t-SNE dimensionality reduction\n",
        "tsne = TSNE(n_components=2)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Create a DataFrame with the 2D t-SNE coordinates\n",
        "d = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOBJdcKuvRpV"
      },
      "source": [
        "For simplicity, we will project the words from the top 1000 instances of the data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ur10QSxAvRpV"
      },
      "outputs": [],
      "source": [
        "d = d.head(1000)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "ax.scatter(d['x'], d['y'])\n",
        "plt.figure(figsize=(500, 500))\n",
        "for word, pos in d.iterrows():\n",
        "  ax.annotate(word, pos)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q08XCacEvRpW"
      },
      "source": [
        "## Question 9:\n",
        "Build a model using the same architecture and use the embeddings that we generated using Word2Vec from gensim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ya7Ma8lgvRpW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXEJOSYAvRpW"
      },
      "source": [
        "## Question 10:\n",
        "Train the architecture using the same split with our own embeddings and print the classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "phDdn4qhvRpW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjjOKk_wvRpW"
      },
      "source": [
        "As we can see, there is still no improvement in performance.\n",
        "\n",
        "## Question 11:\n",
        "Load the model trained over the same architecture using the embeddings trained over 20 epochs. The model is in the location `word2vec_models/w2v_model.hdf5`. Print the classification report for this model."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZUMlWrY80Dc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfaJyduWvRpW"
      },
      "source": [
        "The results imply that the architecture built over the embeddings that we trained on the tweets perform better than using the existing pretrained glove embeddings. Hence it is necessary to build task specific models.\n",
        "\n",
        "## Question 12:\n",
        "Comment of why the embeddings we trained using Word2Vec are better than the pretrained glove embeddings over this particular task of classify hate_speech, offensive_language or neither."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBt9HoAyvRpX"
      },
      "source": [
        "Your Answer:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "B77i1HwevRpX"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}