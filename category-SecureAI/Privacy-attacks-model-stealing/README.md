# Privacy Attacks on Machine Learning Models - Model Stealing
This lab explores model stealing attacks, where an adversary attempts to replicate a machine learning model by querying it and using the responses to train a substitute model. Using the Adversarial Robustness Toolbox (ART), students will implement a model extraction attack to understand how attackers can reconstruct black-box models with minimal knowledge of their internal architecture. The lab highlights the security risks of deploying proprietary machine learning models in public-facing applications, emphasizing the need for defenses against unauthorized model replication.
