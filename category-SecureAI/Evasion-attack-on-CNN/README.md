# Evasion Attacks on CNNs â€“ Crafting Adversarial Examples
This lab explores evasion attacks on Convolutional Neural Networks (CNNs) by generating adversarial examples that mislead a trained classifier. Students will train a CNN using Keras to distinguish between handwritten digits (0 and 1) from the MNIST dataset, preprocess data, and apply adversarial attacks using the Adversarial Robustness Toolbox (ART). The lab focuses on gradient-based attacks, such as Fast Gradient Sign Method (FGSM), to introduce small perturbations that cause misclassification, demonstrating the vulnerabilities of deep learning models to adversarial manipulations.
