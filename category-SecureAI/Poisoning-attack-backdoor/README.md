# Poisoning Attacks using a Backdoor


This lab focuses on poisoning attacks, where an adversary manipulates training data to degrade model performance or insert hidden behaviors. The two main types covered are availability attacks, which aim to make the model unreliable, and integrity (backdoor) attacks, which subtly alter model predictions for specific inputs. Students will explore how poisoning can be used to compromise machine learning models early in the pipeline.
