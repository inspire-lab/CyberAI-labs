{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inspire-lab/CyberAI-labs/blob/main/category-PrivateAI/Training-Secure-MPC/MPC_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZYg9D7kjNA9"
      },
      "source": [
        "# Training an Encrypted Neural Network using CrypTen\n",
        "\n",
        "In this lab, we will walk through an example of how we can train a neural network with CrypTen. This is particularly relevant for the <i>Feature Aggregation</i>, <i>Data Labeling</i> and <i>Data Augmentation</i> use cases. We will focus on the usual two-party setting and show how we can train an accurate neural network for digit classification on the MNIST data.\n",
        "\n",
        "For concreteness, this lab will step through the <i>Feature Aggregation</i> use cases: Alice and Bob each have part of the features of the data set, and wish to train a neural network on their combined data, while keeping their data private.\n",
        "\n",
        "## Setup\n",
        "We'll begin by installing, importing and initializing the `crypten` and `torch` libraries.  \n",
        "\n",
        "We will use the MNIST dataset to demonstrate how Alice and Bob can learn without revealing protected information. For reference, the feature size of each example in the MNIST data is `28 x 28`. Let's assume Alice has the first `28 x 20` features and Bob has last `28 x 8` features. One way to think of this split is that Alice has the (roughly) top 2/3rds of each image, while Bob has the bottom 1/3rd of each image. We'll use helper script `mnist_utils.py` that downloads the publicly available MNIST data, and splits the data as required.\n",
        "\n",
        "For simplicity, we will restrict our problem to binary classification: we'll simply learn how to distinguish between 0 and non-zero digits. For speed of execution in the notebook, we will only create a dataset of a 100 examples."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note: We are installing crypten without dependencies here as the default pip command causes sklearn error."
      ],
      "metadata": {
        "id": "2koT_rB7B2W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision omegaconf>=2.0.6 onnx>=1.7.0 pandas>=1.2.2 pyyaml>=5.3.1 tensorboard future scipy>=1.6.0\n",
        "!pip install --no-deps crypten"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyYiiAGPjVq5",
        "outputId": "f7a00dd7-819e-4e4e-eb6a-249a70260625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crypten\n",
            "  Downloading crypten-0.4.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Downloading crypten-0.4.1-py3-none-any.whl (259 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/259.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m256.0/259.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.9/259.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: crypten\n",
            "Successfully installed crypten-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-88WM77jNBD"
      },
      "outputs": [],
      "source": [
        "import crypten\n",
        "import torch\n",
        "\n",
        "crypten.init()\n",
        "torch.set_num_threads(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvceJ7u-jNBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286cf031-505d-4b48-908d-8db70664c5b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 496kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.53MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.69MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%run ./mnist_utils.py --option features --reduced 100 --binary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkZAZGp2jNBH"
      },
      "source": [
        "Next, we'll define the network architecture below, and then describe how to train it on encrypted data in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHNyQAlEjNBI"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Define an example network\n",
        "class ExampleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ExampleNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=0)\n",
        "        self.fc1 = nn.Linear(16 * 12 * 12, 100)\n",
        "        self.fc2 = nn.Linear(100, 2) # For binary classification, final layer needs only 2 outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = F.relu(out)\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(-1, 16 * 12 * 12)\n",
        "        out = self.fc1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "crypten.common.serial.register_safe_class(ExampleNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihCyXy9JjNBJ"
      },
      "source": [
        "## Encrypted Training\n",
        "\n",
        "We'll first discuss how the training loop in CrypTen differs from PyTorch. Then, we'll go through a complete example to illustrate training on encrypted data from end-to-end.\n",
        "\n",
        "### How does CrypTen training differ from PyTorch training?\n",
        "\n",
        "There are two main ways implementing a CrypTen training loop differs from a PyTorch training loop. We'll describe these items first, and then illustrate them with small examples below.\n",
        "\n",
        "<i>(1) Use one-hot encoding</i>: CrypTen training requires all labels to use one-hot encoding. This means that when using standard datasets such as MNIST, we need to modify the labels to use one-hot encoding.\n",
        "\n",
        "<i>(2) Directly update parameters</i>: CrypTen does not use the PyTorch optimizers. Instead, CrypTen implements encrypted SGD by implementing its own `backward` function, followed by directly updating the parameters. As we will see below, using SGD in CrypTen is very similar to using the PyTorch optimizers.\n",
        "\n",
        "We now show some small examples to illustrate these differences. As before, we will assume Alice has the rank 0 process and Bob has the rank 1 process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oeYOX55jNBM"
      },
      "outputs": [],
      "source": [
        "# Define source argument values for Alice and Bob\n",
        "ALICE = 0\n",
        "BOB = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycoEb1Q7jNBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab32807-54ef-4032-d6ff-12071ec78585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/crypten/__init__.py:334: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  result = load_closure(f, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Load Alice's data\n",
        "data_alice_enc = crypten.load_from_party('./alice_train.pth', src=ALICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPAl8j9KjNBP",
        "outputId": "fdcec8a5-2e20-4c15-89cd-3a9824a8c7d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/crypten/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  param = torch.from_numpy(numpy_helper.to_array(node))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Graph encrypted module"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# We'll now set up the data for our small example below\n",
        "# For illustration purposes, we will create small dataset\n",
        "# and encrypt all of it from source ALICE\n",
        "x_small = torch.rand(100, 1, 28, 28)\n",
        "y_small = torch.randint(1, (100,))\n",
        "\n",
        "# Transform labels into one-hot encoding\n",
        "label_eye = torch.eye(2)\n",
        "y_one_hot = label_eye[y_small]\n",
        "\n",
        "# Transform all data to CrypTensors\n",
        "x_train = crypten.cryptensor(x_small, src=ALICE)\n",
        "y_train = crypten.cryptensor(y_one_hot)\n",
        "\n",
        "# Instantiate and encrypt a CrypTen model\n",
        "model_plaintext = ExampleNet()\n",
        "dummy_input = torch.empty(1, 1, 28, 28)\n",
        "model = crypten.nn.from_pytorch(model_plaintext, dummy_input)\n",
        "model.encrypt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKcSchEGjNBQ",
        "outputId": "d5335371-64ef-4f3b-c12c-0961ee4ee543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 0.5361\n",
            "Epoch: 1 Loss: 0.5044\n",
            "Epoch: 2 Loss: 0.4750\n",
            "Epoch: 3 Loss: 0.4470\n",
            "Epoch: 4 Loss: 0.4209\n",
            "Epoch: 5 Loss: 0.3970\n",
            "Epoch: 6 Loss: 0.3745\n",
            "Epoch: 7 Loss: 0.3536\n",
            "Epoch: 8 Loss: 0.3346\n",
            "Epoch: 9 Loss: 0.3171\n"
          ]
        }
      ],
      "source": [
        "# Example: Stochastic Gradient Descent in CrypTen\n",
        "\n",
        "model.train() # Change to training mode\n",
        "loss = crypten.nn.MSELoss() # Choose loss functions\n",
        "\n",
        "# Set parameters: learning rate, num_epochs\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Train the model: SGD on encrypted data\n",
        "for i in range(num_epochs):\n",
        "\n",
        "    # forward pass\n",
        "    output = model(x_train)\n",
        "    loss_value = loss(output, y_train)\n",
        "\n",
        "    # set gradients to zero\n",
        "    model.zero_grad()\n",
        "\n",
        "    # perform backward pass\n",
        "    loss_value.backward()\n",
        "\n",
        "    # update parameters\n",
        "    #####################\n",
        "    #Your code goes here\n",
        "    #####################\n",
        "\n",
        "    # examine the loss after each epoch\n",
        "    print(\"Epoch: {0:d} Loss: {1:.4f}\".format(i, loss_value.get_plain_text()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-E_1UjojNBQ"
      },
      "source": [
        "### A Complete Example\n",
        "\n",
        "We now put these pieces together for a complete example of training a network in a multi-party setting.\n",
        "\n",
        "We'll assume Alice has the rank 0 process, and Bob has the rank 1 process; so we'll load and encrypt Alice's data with `src=0`, and load and encrypt Bob's data with `src=1`. We'll then initialize a plaintext model and convert it to an encrypted model. We'll finally define our loss function, training parameters, and run SGD on the encrypted data. For the purposes of this lab we train on 100 samples; training should complete in ~3 minutes per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B7gP4swjNBR",
        "outputId": "50d37aaf-77e2-47cb-e13e-dd46a5ca9e18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-1de5a0a0bb6c>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  labels = torch.load('./train_labels.pth')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 28, 20])\n",
            "torch.Size([100, 28, 8])\n",
            "Epoch 0 in progress:\n",
            "\tBatch 1 of 10 Loss 0.4424\n",
            "\tBatch 2 of 10 Loss 0.3554\n",
            "\tBatch 3 of 10 Loss 0.3885\n",
            "\tBatch 4 of 10 Loss 0.3497\n",
            "\tBatch 5 of 10 Loss 0.2915\n",
            "\tBatch 6 of 10 Loss 0.2872\n",
            "\tBatch 7 of 10 Loss 0.2823\n",
            "\tBatch 8 of 10 Loss 0.2705\n",
            "\tBatch 9 of 10 Loss 0.2822\n",
            "\tBatch 10 of 10 Loss 0.2055\n",
            "Epoch 1 in progress:\n",
            "\tBatch 1 of 10 Loss 0.2413\n",
            "\tBatch 2 of 10 Loss 0.1625\n",
            "\tBatch 3 of 10 Loss 0.2095\n",
            "\tBatch 4 of 10 Loss 0.2300\n",
            "\tBatch 5 of 10 Loss 0.1457\n",
            "\tBatch 6 of 10 Loss 0.1758\n",
            "\tBatch 7 of 10 Loss 0.2108\n",
            "\tBatch 8 of 10 Loss 0.1678\n",
            "\tBatch 9 of 10 Loss 0.1996\n",
            "\tBatch 10 of 10 Loss 0.1271\n",
            "Epoch 2 in progress:\n",
            "\tBatch 1 of 10 Loss 0.1560\n",
            "\tBatch 2 of 10 Loss 0.0860\n",
            "\tBatch 3 of 10 Loss 0.1378\n",
            "\tBatch 4 of 10 Loss 0.1842\n",
            "\tBatch 5 of 10 Loss 0.0828\n",
            "\tBatch 6 of 10 Loss 0.1283\n",
            "\tBatch 7 of 10 Loss 0.1825\n",
            "\tBatch 8 of 10 Loss 0.1209\n",
            "\tBatch 9 of 10 Loss 0.1651\n",
            "\tBatch 10 of 10 Loss 0.0941\n",
            "Epoch 3 in progress:\n",
            "\tBatch 1 of 10 Loss 0.1151\n",
            "\tBatch 2 of 10 Loss 0.0526\n",
            "\tBatch 3 of 10 Loss 0.1056\n",
            "\tBatch 4 of 10 Loss 0.1627\n",
            "\tBatch 5 of 10 Loss 0.0537\n",
            "\tBatch 6 of 10 Loss 0.1048\n",
            "\tBatch 7 of 10 Loss 0.1673\n",
            "\tBatch 8 of 10 Loss 0.0961\n",
            "\tBatch 9 of 10 Loss 0.1470\n",
            "\tBatch 10 of 10 Loss 0.0776\n",
            "Epoch 4 in progress:\n",
            "\tBatch 1 of 10 Loss 0.0929\n",
            "\tBatch 2 of 10 Loss 0.0369\n",
            "\tBatch 3 of 10 Loss 0.0887\n",
            "\tBatch 4 of 10 Loss 0.1496\n",
            "\tBatch 5 of 10 Loss 0.0389\n",
            "\tBatch 6 of 10 Loss 0.0900\n",
            "\tBatch 7 of 10 Loss 0.1562\n",
            "\tBatch 8 of 10 Loss 0.0807\n",
            "\tBatch 9 of 10 Loss 0.1350\n",
            "\tBatch 10 of 10 Loss 0.0677\n",
            "Epoch 5 in progress:\n",
            "\tBatch 1 of 10 Loss 0.0790\n",
            "\tBatch 2 of 10 Loss 0.0288\n",
            "\tBatch 3 of 10 Loss 0.0785\n",
            "\tBatch 4 of 10 Loss 0.1395\n",
            "\tBatch 5 of 10 Loss 0.0307\n",
            "\tBatch 6 of 10 Loss 0.0791\n",
            "\tBatch 7 of 10 Loss 0.1469\n",
            "\tBatch 8 of 10 Loss 0.0698\n",
            "\tBatch 9 of 10 Loss 0.1254\n",
            "\tBatch 10 of 10 Loss 0.0609\n",
            "Epoch 6 in progress:\n",
            "\tBatch 1 of 10 Loss 0.0692\n",
            "\tBatch 2 of 10 Loss 0.0242\n",
            "\tBatch 3 of 10 Loss 0.0714\n",
            "\tBatch 4 of 10 Loss 0.1309\n",
            "\tBatch 5 of 10 Loss 0.0257\n",
            "\tBatch 6 of 10 Loss 0.0704\n",
            "\tBatch 7 of 10 Loss 0.1377\n",
            "\tBatch 8 of 10 Loss 0.0610\n",
            "\tBatch 9 of 10 Loss 0.1170\n",
            "\tBatch 10 of 10 Loss 0.0555\n",
            "Epoch 7 in progress:\n",
            "\tBatch 1 of 10 Loss 0.0618\n",
            "\tBatch 2 of 10 Loss 0.0212\n",
            "\tBatch 3 of 10 Loss 0.0660\n",
            "\tBatch 4 of 10 Loss 0.1225\n",
            "\tBatch 5 of 10 Loss 0.0224\n",
            "\tBatch 6 of 10 Loss 0.0629\n",
            "\tBatch 7 of 10 Loss 0.1292\n",
            "\tBatch 8 of 10 Loss 0.0541\n",
            "\tBatch 9 of 10 Loss 0.1095\n",
            "\tBatch 10 of 10 Loss 0.0510\n",
            "Epoch 8 in progress:\n",
            "\tBatch 1 of 10 Loss 0.0557\n",
            "\tBatch 2 of 10 Loss 0.0190\n",
            "\tBatch 3 of 10 Loss 0.0615\n",
            "\tBatch 4 of 10 Loss 0.1149\n",
            "\tBatch 5 of 10 Loss 0.0200\n",
            "\tBatch 6 of 10 Loss 0.0565\n",
            "\tBatch 7 of 10 Loss 0.1218\n",
            "\tBatch 8 of 10 Loss 0.0482\n",
            "\tBatch 9 of 10 Loss 0.1031\n",
            "\tBatch 10 of 10 Loss 0.0470\n",
            "Epoch 9 in progress:\n",
            "\tBatch 1 of 10 Loss 0.0508\n",
            "\tBatch 2 of 10 Loss 0.0174\n",
            "\tBatch 3 of 10 Loss 0.0577\n",
            "\tBatch 4 of 10 Loss 0.1080\n",
            "\tBatch 5 of 10 Loss 0.0182\n",
            "\tBatch 6 of 10 Loss 0.0510\n",
            "\tBatch 7 of 10 Loss 0.1149\n",
            "\tBatch 8 of 10 Loss 0.0430\n",
            "\tBatch 9 of 10 Loss 0.0971\n",
            "\tBatch 10 of 10 Loss 0.0434\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import crypten.mpc as mpc\n",
        "import crypten.communicator as comm\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "# Since labels are public in this use case, we will simply use them from loaded torch tensors\n",
        "labels = torch.load('./train_labels.pth')\n",
        "labels = labels.long()\n",
        "labels_one_hot = label_eye[labels]\n",
        "\n",
        "@mpc.run_multiprocess(world_size=2)\n",
        "def run_encrypted_training():\n",
        "    # Load data:\n",
        "    x_alice_enc = crypten.load_from_party('./alice_train.pth', src=ALICE)\n",
        "    x_bob_enc = crypten.load_from_party('./bob_train.pth', src=BOB)\n",
        "\n",
        "    crypten.print(x_alice_enc.size())\n",
        "    crypten.print(x_bob_enc.size())\n",
        "\n",
        "    # Combine the feature sets\n",
        "    x_combined_enc = crypten.cat([x_alice_enc, x_bob_enc], dim=2)\n",
        "\n",
        "    # Reshape to match the network architecture\n",
        "    x_combined_enc = x_combined_enc.unsqueeze(1)\n",
        "\n",
        "\n",
        "    # Commenting out due to intermittent failure in PyTorch codebase\n",
        "\n",
        "    # Initialize a plaintext model and convert to CrypTen model\n",
        "    pytorch_model = ExampleNet()\n",
        "    model =\n",
        "\n",
        "    #####################\n",
        "    #Your code goes here\n",
        "    #####################\n",
        "    model.encrypt()\n",
        "    # Set train mode\n",
        "    model.train()\n",
        "\n",
        "    # Define a loss function\n",
        "    loss = crypten.nn.MSELoss()\n",
        "\n",
        "    # Define training parameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    batch_size = 10\n",
        "    num_batches = x_combined_enc.size(0) // batch_size\n",
        "\n",
        "    rank = comm.get().get_rank()\n",
        "    for i in range(num_epochs):\n",
        "        crypten.print(f\"Epoch {i} in progress:\")\n",
        "\n",
        "        for batch in range(num_batches):\n",
        "            # define the start and end of the training mini-batch\n",
        "            start, end = batch * batch_size, (batch + 1) * batch_size\n",
        "\n",
        "            # construct CrypTensors out of training examples / labels\n",
        "            x_train = x_combined_enc[start:end]\n",
        "            y_batch = labels_one_hot[start:end]\n",
        "            y_train = crypten.cryptensor(y_batch, requires_grad=True)\n",
        "\n",
        "            # perform forward pass:\n",
        "            output = model(x_train)\n",
        "            loss_value = loss(output, y_train)\n",
        "\n",
        "            # set gradients to \"zero\"\n",
        "            model.zero_grad()\n",
        "\n",
        "            # perform backward pass:\n",
        "            loss_value.backward()\n",
        "\n",
        "            # update parameters\n",
        "            model.update_parameters(learning_rate)\n",
        "\n",
        "            # Print progress every batch:\n",
        "            batch_loss = loss_value.get_plain_text()\n",
        "            crypten.print(f\"\\tBatch {(batch + 1)} of {num_batches} Loss {batch_loss.item():.4f}\")\n",
        "\n",
        "run_encrypted_training()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9QoeKgfjNBS"
      },
      "source": [
        "We see that the average batch loss decreases across the epochs, as we expect during training."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. https://crypten.ai/\n",
        "2. https://github.com/facebookresearch/CrypTen"
      ],
      "metadata": {
        "id": "_32Bq3wj3fXS"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}