# Differential Privacy - DP-SGD

This lab focuses on Differentially Private Stochastic Gradient Descent (DP-SGD), a technique for training neural networks while preserving data privacy. Using TensorFlow Privacy, students will train a Convolutional Neural Network (CNN) on MNIST with DP-SGD, which adds calibrated noise to gradients during optimization to prevent memorization of individual data points. The lab highlights how differential privacy can be integrated into deep learning to ensure privacy-preserving model training.
